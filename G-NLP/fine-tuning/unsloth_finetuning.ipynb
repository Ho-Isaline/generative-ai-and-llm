{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b955a8d-9389-460a-b8f2-c7913eb2c484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\unsloth\\__init__.py:103\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m# Try loading bitsandbytes and triton\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtriton\u001b[39;00m\n\u001b[0;32m    104\u001b[0m libcuda_dirs \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m: \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m Version(triton\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m3.0.0\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import PeftModel\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Configuration settings\n",
    "max_seq_length = 2048\n",
    "dtype = None  # None for auto-detection; set to Float16 or Bfloat16 based on GPU\n",
    "load_in_4bit = True\n",
    "\n",
    "# Define model path\n",
    "base_model_name = \"/home/jj/Meta-Llama-3.1-8B-bnb-4bit/\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Apply PEFT for memory-efficient training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Define prompt format for training\n",
    "training_prompt_template = \"\"\"Please reply to the following question:\n",
    "\n",
    "### Prompt:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    return {\n",
    "        \"text\": [\n",
    "            training_prompt_template.format(prompt, completion) + EOS_TOKEN\n",
    "            for prompt, completion in zip(examples[\"prompt\"], examples[\"completion\"])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Load and format the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"./sample_dataset.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Check if bfloat16 is supported\n",
    "def is_bfloat16_supported():\n",
    "    return torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "# Configure and create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Define the path to save the trained model and tokenizer\n",
    "save_path = \"outputs/final_model\"\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")\n",
    "\n",
    "# Load the base model for inference after training\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Adjust generation parameters for varied responses\n",
    "def generate_completion(prompt, model, tokenizer, max_length=150, temperature=0.8):\n",
    "    # Prepare the input for generation\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage of the generate_completion function\n",
    "prompt = \"Explain what Docker is.\"\n",
    "generated_completion = generate_completion(prompt, model, tokenizer)\n",
    "print(f\"Generated Response:\\n{generated_completion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873313a5-a9d3-4d3f-a63d-10c17e88dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Casual Response:\n",
      "Docker is like a containerization platform that helps developers and IT professionals package applications into standardized units. It makes it easier to develop, deploy, and run applications.\n",
      "\n",
      "Humorous Response:\n",
      "Docker simplifies the development and deployment process by providing consistency across multiple development environments. It helps to ensure that your application will behave the same way regardless of where it is deployed.\n",
      "\n",
      "Technical Response:\n",
      "Docker is an open platform for developing, shipping, and running applications. It allows developers to package applications into containers—standardized executable components that combine application source code with the operating system (OS) libraries and dependencies required to run that code in any environment.\n",
      "\n",
      "Docker provides a way to deliver software in packages that include everything it needs to run—including the code, runtime, libraries, and system settings. The result is faster deployment, which means applications can be shipped quickly.\n",
      "\n",
      "Storytelling Response:\n",
      "Docker simplifies the development process by providing a consistent environment for building and running applications. It helps developers to package applications into containers—standardized executable components that include the code, runtime, libraries, and system settings.\n",
      "\n",
      "Neutral Response:\n",
      "Docker is an open platform for developing, shipping, and running applications. It allows developers to package applications into containers—standardized executable components that combine application source code with the operating system (OS) libraries and dependencies required to run that code in any environment.\n"
     ]
    }
   ],
   "source": [
    "def generate_completion(prompt, model, tokenizer, max_length=150, temperature=0.8, style=\"neutral\"):\n",
    "    # More explicit style-specific prompts\n",
    "    style_prompts = {\n",
    "        \"casual\": \"Hey, can you explain Docker in a super chill, laid-back way? Like you're chatting with a friend over coffee.\",\n",
    "        \"humorous\": \"Alright, time to get silly! Explain Docker like you're a stand-up comedian doing a bit about container technology. Make it absurdly funny!\",\n",
    "        \"technical\": \"Provide a detailed technical explanation of Docker, focusing on its architecture, components, and how it interfaces with the host system. Use precise terminology.\",\n",
    "        \"storytelling\": \"Tell me a short story about a developer named Alex who discovers the magic of Docker. Make it engaging and narrative-driven, while still explaining what Docker does.\",\n",
    "        \"neutral\": prompt  # Default to the original prompt if no style is specified\n",
    "    }\n",
    "    \n",
    "    styled_prompt = f\"{style_prompts.get(style.lower(), prompt)}\\n\\nExplanation:\"\n",
    "    \n",
    "    inputs = tokenizer(styled_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove any repetition of the prompt\n",
    "    completion = completion.replace(styled_prompt, \"\").strip()\n",
    "    return completion\n",
    "\n",
    "# Example usage\n",
    "styles = [\"casual\", \"humorous\", \"technical\", \"storytelling\", \"neutral\"]\n",
    "for style in styles:\n",
    "    response = generate_completion(\"What is Docker?\", model, tokenizer, style=style)\n",
    "    print(f\"\\n{style.capitalize()} Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbfe75-17bf-41e7-a827-5a1baf2ed25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1d2a9ecc905ef171fa753a7e952cc3d53b061ba87f364110eb501a5ad998ef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
