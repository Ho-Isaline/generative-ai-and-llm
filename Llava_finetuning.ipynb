{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba3b18b-ebf7-4eae-b8db-e4a62003ee49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [02:32<00:00, 50.99s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './llava/clothing_style_dataset.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[0;32m---> 70\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./llava/clothing_style_dataset.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Set training arguments\u001b[39;00m\n\u001b[1;32m     73\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     74\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-clothing-style-identification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.4e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     bf16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     85\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_jsonl\u001b[39m(file_path):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n",
      "File \u001b[0;32m~/miniconda3/envs/group2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './llava/clothing_style_dataset.jsonl'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model from the local directory (4-bit quantized)\n",
    "model_path = \"/home/jj/llava-1.5-7b-hf\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_path,\n",
    "                                                      quantization_config=quantization_config,\n",
    "                                                      torch_dtype=torch.float16)\n",
    "\n",
    "# Define a chat template and set the tokenizer and processor\n",
    "LLAVA_CHAT_TEMPLATE = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. {% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "# Data collator for processing text and image pairs\n",
    "class LLavaDataCollator:\n",
    "    def __init__(self, processor, img_dir):\n",
    "        self.processor = processor\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            # Prepare the conversation as a template\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the clothing style in this image?\"}, {\"type\": \"image\"}]},\n",
    "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"response\"]}]}\n",
    "            ]\n",
    "            text = self.processor.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "            \n",
    "            # Load and process the image\n",
    "            full_image_path = os.path.join(self.img_dir, os.path.basename(example[\"image_path\"]))\n",
    "            image = Image.open(full_image_path).convert('RGB')\n",
    "            images.append(image)\n",
    "\n",
    "        # Process the batch\n",
    "        inputs = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Prepare the labels for supervised fine-tuning\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        if self.processor.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        inputs[\"labels\"] = labels\n",
    "\n",
    "        return inputs\n",
    "\n",
    "img_dir = \"./llava/img\"\n",
    "data_collator = LLavaDataCollator(processor, img_dir)\n",
    "\n",
    "# Function to load JSONL dataset\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "train_dataset = load_jsonl(\"./llava/clothing_style_dataset.jsonl\")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llava-clothing-style-identification\",\n",
    "    learning_rate=1.4e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs=100,\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    bf16=False\n",
    ")\n",
    "\n",
    "# LoRA configuration for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\"  # Applies LoRA to all linear layers\n",
    ")\n",
    "\n",
    "# Create the SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",  # Dummy field as required by SFTTrainer\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model locally\n",
    "trainer.save_model(\"./fine_tuned_llava\")\n",
    "print(\"Fine-tuned model saved to ./fine_tuned_llava\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffbba40-31b2-461d-84d0-300b8b92d933",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./llava.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./llava/img/1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the base (untrained) model WITHOUT 4-bit quantization\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using float16 for GPU efficiency\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m base_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned model WITH 4-bit quantization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/group2/lib/python3.10/site-packages/transformers/modeling_utils.py:3763\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3759\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3760\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3761\u001b[0m         )\n\u001b[1;32m   3762\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3764\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3765\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3766\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3767\u001b[0m         )\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3769\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./llava."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to the base model, fine-tuned model, and image\n",
    "base_model_path = \"./llava\"\n",
    "fine_tuned_adapter_path = \"./fine_tuned_llava\"\n",
    "image_path = \"./llava/img/1.jpg\"\n",
    "\n",
    "# Load the base (untrained) model WITHOUT 4-bit quantization\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,  # Using float16 for GPU efficiency\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the fine-tuned model WITH 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "fine_tuned_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,  # Using float16 with 4-bit quantization for memory efficiency\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned adapter into the quantized model\n",
    "fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, fine_tuned_adapter_path)\n",
    "fine_tuned_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load tokenizer and processor for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "processor = AutoProcessor.from_pretrained(base_model_path)\n",
    "\n",
    "# Function to generate response from the model based on the image and prompt\n",
    "def generate_response(model, image_path, prompt=\"What is the clothing style in this image?\"):\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Prepare input with the prompt and image placeholder\n",
    "    prompt_with_image = f\"{prompt}\\n<image>\"\n",
    "    inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\").to('cuda')  # Send input to GPU\n",
    "    \n",
    "    # Generate the response without computing gradients (no training)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    # Decode the generated response from the model\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the response by removing the prompt and assistant marker\n",
    "    response = response.split(prompt)[-1].strip()\n",
    "    response = response.replace(\"ASSISTANT:\", \"\").strip()\n",
    "\n",
    "    # Remove any common end tokens from the response\n",
    "    end_tokens = [\"</s>\", \"<|endoftext|>\", \"<|end|>\"]\n",
    "    for token in end_tokens:\n",
    "        response = response.split(token)[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example prompt for generating responses from both models\n",
    "prompt = \"What is the clothing style in this image?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbe7fb3-d236-4150-af88-73a98c357032",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Original base model response (float16, no quantization)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m base_model_response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m(base_model, image_path, prompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBase Model Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_model_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Original base model response (float16, no quantization)\n",
    "base_model_response = generate_response(base_model, image_path, prompt)\n",
    "print(f\"Base Model Response: {base_model_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab4e535-7b51-40ca-acbc-9a8fdcf2cc70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tuned model response (4-bit quantized)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fine_tuned_model_response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m(fine_tuned_model, image_path, prompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuned Model Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_tuned_model_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model response (4-bit quantized)\n",
    "fine_tuned_model_response = generate_response(fine_tuned_model, image_path, prompt)\n",
    "print(f\"Fine-tuned Model Response: {fine_tuned_model_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17db50f7-5e66-48b2-9f59-bbc68cc5c352",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./llava/img/1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_image\u001b[39m(image_path):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Open the image\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = \"./llava/img/1.jpg\"\n",
    "\n",
    "def display_image(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Set up the display with matplotlib\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    \n",
    "    # Show the image\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the image\n",
    "display_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605750a-f57f-4beb-aa2f-bf8a674bf20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
