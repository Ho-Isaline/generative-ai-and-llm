{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3b18b-ebf7-4eae-b8db-e4a62003ee49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model from the local directory (4-bit quantized)\n",
    "model_path = \"/home/jj/llava-1.5-7b-hf\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_path,\n",
    "                                                      quantization_config=quantization_config,\n",
    "                                                      torch_dtype=torch.float16)\n",
    "\n",
    "# Define a chat template and set the tokenizer and processor\n",
    "LLAVA_CHAT_TEMPLATE = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. {% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "# Data collator for processing text and image pairs\n",
    "class LLavaDataCollator:\n",
    "    def __init__(self, processor, img_dir):\n",
    "        self.processor = processor\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            # Prepare the conversation as a template\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the clothing style in this image?\"}, {\"type\": \"image\"}]},\n",
    "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"response\"]}]}\n",
    "            ]\n",
    "            text = self.processor.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "            \n",
    "            # Load and process the image\n",
    "            full_image_path = os.path.join(self.img_dir, os.path.basename(example[\"image_path\"]))\n",
    "            image = Image.open(full_image_path).convert('RGB')\n",
    "            images.append(image)\n",
    "\n",
    "        # Process the batch\n",
    "        inputs = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Prepare the labels for supervised fine-tuning\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        if self.processor.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        inputs[\"labels\"] = labels\n",
    "\n",
    "        return inputs\n",
    "\n",
    "img_dir = \"./llava/img\"\n",
    "data_collator = LLavaDataCollator(processor, img_dir)\n",
    "\n",
    "# Function to load JSONL dataset\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "train_dataset = load_jsonl(\"./llava/clothing_style_dataset.jsonl\")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llava-clothing-style-identification\",\n",
    "    learning_rate=1.4e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs=100,\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    bf16=False\n",
    ")\n",
    "\n",
    "# LoRA configuration for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\"  # Applies LoRA to all linear layers\n",
    ")\n",
    "\n",
    "# Create the SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",  # Dummy field as required by SFTTrainer\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model locally\n",
    "trainer.save_model(\"./fine_tuned_llava\")\n",
    "print(\"Fine-tuned model saved to ./fine_tuned_llava\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbba40-31b2-461d-84d0-300b8b92d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to the base model, fine-tuned model, and image\n",
    "base_model_path = \"./llava\"\n",
    "fine_tuned_adapter_path = \"./fine_tuned_llava\"\n",
    "image_path = \"./llava/img/1.jpg\"\n",
    "\n",
    "# Load the base (untrained) model WITHOUT 4-bit quantization\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,  # Using float16 for GPU efficiency\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the fine-tuned model WITH 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "fine_tuned_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,  # Using float16 with 4-bit quantization for memory efficiency\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned adapter into the quantized model\n",
    "fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, fine_tuned_adapter_path)\n",
    "fine_tuned_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load tokenizer and processor for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "processor = AutoProcessor.from_pretrained(base_model_path)\n",
    "\n",
    "# Function to generate response from the model based on the image and prompt\n",
    "def generate_response(model, image_path, prompt=\"What is the clothing style in this image?\"):\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Prepare input with the prompt and image placeholder\n",
    "    prompt_with_image = f\"{prompt}\\n<image>\"\n",
    "    inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\").to('cuda')  # Send input to GPU\n",
    "    \n",
    "    # Generate the response without computing gradients (no training)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    # Decode the generated response from the model\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the response by removing the prompt and assistant marker\n",
    "    response = response.split(prompt)[-1].strip()\n",
    "    response = response.replace(\"ASSISTANT:\", \"\").strip()\n",
    "\n",
    "    # Remove any common end tokens from the response\n",
    "    end_tokens = [\"</s>\", \"<|endoftext|>\", \"<|end|>\"]\n",
    "    for token in end_tokens:\n",
    "        response = response.split(token)[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example prompt for generating responses from both models\n",
    "prompt = \"What is the clothing style in this image?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe7fb3-d236-4150-af88-73a98c357032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original base model response (float16, no quantization)\n",
    "base_model_response = generate_response(base_model, image_path, prompt)\n",
    "print(f\"Base Model Response: {base_model_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4e535-7b51-40ca-acbc-9a8fdcf2cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model response (4-bit quantized)\n",
    "fine_tuned_model_response = generate_response(fine_tuned_model, image_path, prompt)\n",
    "print(f\"Fine-tuned Model Response: {fine_tuned_model_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db50f7-5e66-48b2-9f59-bbc68cc5c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = \"./llava/img/1.jpg\"\n",
    "\n",
    "def display_image(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Set up the display with matplotlib\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    \n",
    "    # Show the image\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the image\n",
    "display_image(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
