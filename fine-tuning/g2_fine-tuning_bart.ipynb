{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5dcb19-5d82-4495-bebb-4625e791d61b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Informal language fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new PEFT model\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:12<00:00,  2.27it/s, loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 12.0772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:11<00:00,  2.34it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Loss: 0.5660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:12<00:00,  2.33it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Loss: 0.3248\n",
      "Stage 2: Bart Simpson-specific fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 91 examples [00:00, 17598.75 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:00<00:00, 828.14 examples/s]\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new PEFT model\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:39<00:00,  2.32it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 4.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:39<00:00,  2.33it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 0.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:38<00:00,  2.36it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 0.3053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:39<00:00,  2.32it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 0.2464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:39<00:00,  2.31it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 0.1984\n",
      "Fine-tuning completed. Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepare_dataset(file_path, tokenizer, max_length=512):\n",
    "    dataset = load_dataset('json', data_files=file_path)['train']\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        if 'prompt' in examples and 'completion' in examples:\n",
    "            prompts = [f\"Human: {q}\\nSpongebob:\" for q in examples['prompt']]\n",
    "            responses = examples['completion']\n",
    "        else:\n",
    "            raise KeyError(\"The dataset must have 'prompt' and 'completion' fields.\")\n",
    "        \n",
    "        inputs = tokenizer(prompts, responses,\n",
    "                           truncation=True, max_length=max_length,\n",
    "                           padding=\"max_length\", return_tensors=\"pt\")\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            prompt_length = len(tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "            inputs[\"labels\"][i][:prompt_length] = -100\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    tokenized_dataset.set_format(type='torch')\n",
    "    \n",
    "    # # Debug print\n",
    "    # print(\"Dataset structure:\")\n",
    "    # print(tokenized_dataset[0])\n",
    "    # print(\"Dataset length:\", len(tokenized_dataset))\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "def load_model(model_name, peft_config, peft_model_path=None):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    if peft_model_path and os.path.exists(os.path.join(peft_model_path, \"adapter_model.safetensors\")):\n",
    "        print(f\"Loading PEFT model from {peft_model_path}\")\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "        adapter_weights = load_file(os.path.join(peft_model_path, \"adapter_model.safetensors\"))\n",
    "        model.load_state_dict(adapter_weights, strict=False)\n",
    "    else:\n",
    "        print(\"Creating new PEFT model\")\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "def fine_tune(model, dataset, output_dir, num_epochs, batch_size=1, learning_rate=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=100, num_training_steps=len(dataloader) * num_epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model.save_pretrained(os.path.join(output_dir, f\"checkpoint-epoch-{epoch+1}\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    model_name = \"/home/jj/Meta-Llama-3.1-8B-bnb-4bit/\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    # Stage 1: Informal language fine-tuning\n",
    "    informal_model_path = \"./informal_finetuned\"\n",
    "    if not os.path.exists(os.path.join(informal_model_path, \"adapter_model.safetensors\")):\n",
    "        print(\"Stage 1: Informal language fine-tuning\")\n",
    "        informal_dataset = prepare_dataset('informal.jsonl', tokenizer)\n",
    "        model = load_model(model_name, peft_config)\n",
    "        model = fine_tune(model, informal_dataset, informal_model_path, num_epochs=3)\n",
    "    else:\n",
    "        print(\"Skipping Stage 1: Informal fine-tuned model already exists\")\n",
    "\n",
    "    # Stage 2: Bart Simpson-specific fine-tuning\n",
    "    print(\"Stage 2: Bart Simpson-specific fine-tuning\")\n",
    "    bart_dataset = prepare_dataset('spongebob.jsonl', tokenizer)\n",
    "    \n",
    "    bart_model_path = \"./bart_finetuned\"\n",
    "    model = load_model(model_name, peft_config, informal_model_path)\n",
    "    model = fine_tune(model, bart_dataset, bart_model_path, num_epochs=5)\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save_pretrained(bart_model_path)\n",
    "    tokenizer.save_pretrained(bart_model_path)\n",
    "    \n",
    "    print(\"Fine-tuning completed. Model saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad5f8b-02c3-4c9c-906c-0d98c0dad8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/miniconda3/envs/group2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. You can now ask Docker-related questions. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  what is docker?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpongeBob: Docker is like a secret clubhouse for all your cool stuff! You can put in all the things you like, like your games and music, and then you can share them with your friends! It's so fun!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    # Load the configuration\n",
    "    config = PeftConfig.from_pretrained(model_path)\n",
    "    \n",
    "    # Load the base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name_or_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load the fine-tuned model\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_length=200):\n",
    "    # Prepare the input\n",
    "    input_ids = tokenizer.encode(f\"Human: {prompt}\\nSpongebob:\", return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    response = response.split(\"Spongebob:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    # Path to your fine-tuned model\n",
    "    model_path = \"./bart_finetuned\"\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    \n",
    "    print(\"Model loaded. You can now ask Docker-related questions. Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = generate_response(model, tokenizer, user_input)\n",
    "        print(f\"SpongeBob: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1d2a9ecc905ef171fa753a7e952cc3d53b061ba87f364110eb501a5ad998ef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
